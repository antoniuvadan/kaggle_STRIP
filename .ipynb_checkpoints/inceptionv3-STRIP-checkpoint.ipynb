{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832b40cd-ab9d-43bb-bf4d-de1218998b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import datetime as dt\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.transforms as tsfm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072b6bc8-12ac-45a0-b7e4-fdc49fc76033",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"Inception3\", \"InceptionOutputs\", \"_InceptionOutputs\", \"Inception_V3_Weights\", \"inception_v3\"]\n",
    "\n",
    "\n",
    "InceptionOutputs = namedtuple(\"InceptionOutputs\", [\"logits\", \"aux_logits\"])\n",
    "InceptionOutputs.__annotations__ = {\"logits\": Tensor, \"aux_logits\": Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _InceptionOutputs set here for backwards compat\n",
    "_InceptionOutputs = InceptionOutputs\n",
    "\n",
    "\n",
    "class Inception3STRIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 1000,\n",
    "        aux_logits: bool = True,\n",
    "        transform_input: bool = False,\n",
    "        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
    "        init_weights: Optional[bool] = None,\n",
    "        dropout: float = 0.5,\n",
    "        handcrafted_feature_size: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if inception_blocks is None:\n",
    "            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn(\n",
    "                \"The default weight initialization of inception_v3 will be changed in future releases of \"\n",
    "                \"torchvision. If you wish to keep the old behavior (which leads to long initialization times\"\n",
    "                \" due to scipy/scipy#11299), please set init_weights=True.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            init_weights = True\n",
    "        if len(inception_blocks) != 7:\n",
    "            raise ValueError(f\"length of inception_blocks should be 7 instead of {len(inception_blocks)}\")\n",
    "        conv_block = inception_blocks[0]\n",
    "        inception_a = inception_blocks[1]\n",
    "        inception_b = inception_blocks[2]\n",
    "        inception_c = inception_blocks[3]\n",
    "        inception_d = inception_blocks[4]\n",
    "        inception_e = inception_blocks[5]\n",
    "        inception_aux = inception_blocks[6]\n",
    "        \n",
    "        self.num_classes_STRIP = 2\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
    "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
    "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
    "        self.Mixed_6a = inception_b(288)\n",
    "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
    "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
    "        self.AuxLogits: Optional[nn.Module] = None\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = inception_aux(768, self.num_classes_STRIP)\n",
    "        self.Mixed_7a = inception_d(768)\n",
    "        self.Mixed_7b = inception_e(1280)\n",
    "        self.Mixed_7c = inception_e(2048)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Rename fc layer so pretrained weights don't get assigned to it when initializing the net.\n",
    "        self.fc_0 = nn.Linear(2048 + handcrafted_feature_size, 128)\n",
    "        self.fc_1 = nn.Linear(128, self.num_classes_STRIP)\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    stddev = float(m.stddev) if hasattr(m, \"stddev\") else 0.1  # type: ignore\n",
    "                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x: Tensor) -> Tensor:\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x: Tensor, feature: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = self.maxpool2(x)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        aux: Optional[Tensor] = None\n",
    "        if self.AuxLogits is not None:\n",
    "            if self.training:\n",
    "                aux = self.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = self.avgpool(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        feature = torch.flatten(feature, 1)\n",
    "        x = torch.cat((x, feature), dim=1) # concat handcrafted features and img features\n",
    "        x = self.dropout(x) # as per Mardanisamani et al.'s implementation.\n",
    "        x = self.fc_0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x) # as per Mardanisamani et al.'s implementation.\n",
    "        x = self.fc_1(x)\n",
    "\n",
    "        # N x num_classes\n",
    "        return x, aux\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]) -> InceptionOutputs:\n",
    "        if self.training and self.aux_logits:\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return x  # type: ignore[return-value]\n",
    "\n",
    "    def forward(self, x: Tensor, feature: Tensor) -> InceptionOutputs:\n",
    "        x = self._transform_input(x)\n",
    "        x, aux = self._forward(x, feature)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, pool_features: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionB(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionC(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionD(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, num_classes: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes_STRIP = 2\n",
    "        \n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01  # type: ignore[assignment]\n",
    "        self.fc_0 = nn.Linear(768, self.num_classes_STRIP)\n",
    "        self.fc_0.stddev = 0.001  # type: ignore[assignment]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # N x 768 x 17 x 17\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        # N x 768 x 5 x 5\n",
    "        x = self.conv0(x)\n",
    "        # N x 128 x 5 x 5\n",
    "        x = self.conv1(x)\n",
    "        # N x 768 x 1 x 1\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 768 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 768\n",
    "        x = self.fc_0(x)\n",
    "        # N x 1000\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class Inception_V3_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=299, resize_size=342),\n",
    "        meta={\n",
    "            \"num_params\": 27161264,\n",
    "            \"min_size\": (75, 75),\n",
    "            \"categories\": _IMAGENET_CATEGORIES,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#inception-v3\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 77.294,\n",
    "                    \"acc@5\": 93.450,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 5.713,\n",
    "            \"_file_size\": 103.903,\n",
    "            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "# @register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Inception_V3_Weights.IMAGENET1K_V1))\n",
    "def inception_v3STRIP(*, \n",
    "                      weights: Optional[Inception_V3_Weights] = None, \n",
    "                      progress: bool = True, \n",
    "                      handcrafted_feature_size: int = 0, \n",
    "                      **kwargs: Any\n",
    "                     ) -> Inception3STRIP:\n",
    "    \"\"\"\n",
    "    Inception v3 model architecture from\n",
    "    `Rethinking the Inception Architecture for Computer Vision <http://arxiv.org/abs/1512.00567>`_.\n",
    "\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Inception_V3_Weights`, optional): The\n",
    "            pretrained weights for the model. See\n",
    "            :class:`~torchvision.models.Inception_V3_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.Inception3``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Inception_V3_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Inception_V3_Weights.verify(weights)\n",
    "\n",
    "    original_aux_logits = kwargs.get(\"aux_logits\", True)\n",
    "    if weights is not None:\n",
    "        if \"transform_input\" not in kwargs:\n",
    "            _ovewrite_named_param(kwargs, \"transform_input\", True)\n",
    "            _ovewrite_named_param(kwargs, \"aux_logits\", True)\n",
    "            _ovewrite_named_param(kwargs, \"init_weights\", False)\n",
    "            _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = Inception3STRIP(handcrafted_feature_size=handcrafted_feature_size, **kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress), strict=False)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.AuxLogits = None\n",
    "    \n",
    "    #TODO: freeze weights of all layers except last one.\n",
    "    # I.e. freeze layers model_ft.children()[0:-1]\n",
    "    '''\n",
    "    # Example:\n",
    "    https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088/2\n",
    "\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    ct = 0\n",
    "    for child in model_ft.children():\n",
    "    ct += 1\n",
    "    if ct < 7:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    '''\n",
    "            \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38698aa-d288-47c8-839a-31e1c7173f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_valid_test_datasets(train_dataset, valid_dataset, test_dataset, \n",
    "                                   batch_size, num_workers=0):\n",
    "    dataloaders = dict()\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                       shuffle=True,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       num_workers=num_workers)\n",
    "    dataloaders['valid'] = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                                       shuffle=True,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       num_workers=num_workers)\n",
    "    dataloaders['test'] = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                      shuffle=False,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      num_workers=num_workers)\n",
    "    return dataloaders\n",
    "\n",
    "    \n",
    "\n",
    "class STRIPKaggleDataset(Dataset):\n",
    "    def __init__(self, data_dir, lbp_df, hoc_df,\n",
    "                 transform=None, feature_transform=None):\n",
    "        \"\"\"\n",
    "        data_dir: path to directory containing subdirectories, each containing images\n",
    "                  for one class of the dataset.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.feature_transform = feature_transform\n",
    "        self.data_dir = data_dir\n",
    "        classes = ['CE', 'LAA']\n",
    "        self.cat2id = {'CE': 0, 'LAA': 1}\n",
    "        self.id2cat = {0: 'CE', 1: 'LAA'}\n",
    "\n",
    "        self.image_pths = []\n",
    "        self.catids = []\n",
    "        self.class_size = {}\n",
    "        self.features = []\n",
    "        for i, cat in enumerate(classes):\n",
    "            cat_dir = os.path.join(self.data_dir, cat)\n",
    "            img_adrses = sorted([os.path.join(self.data_dir, cat, img_adrs)\n",
    "                                 for img_adrs in os.listdir(cat_dir)\n",
    "                                 if (os.path.isfile(os.path.join(cat_dir, img_adrs))\n",
    "                                     and img_adrs.endswith('.png'))])\n",
    "            self.class_size[cat] = len(img_adrses)\n",
    "            self.catids.extend([i] * self.class_size[cat])\n",
    "            self.image_pths.extend(sorted(img_adrses))\n",
    "            \n",
    "            for adrs in sorted(img_adrses):\n",
    "                img_name = ''\n",
    "                tile_name = ''\n",
    "                tile_file_name = adrs.split('/')[-1].removesuffix('.png')\n",
    "\n",
    "                # Break tile file name into image name and tile name.\n",
    "                is_second = False # is second '_' character\n",
    "                for i, l in enumerate(tile_file_name):\n",
    "                    if l == '_' and not is_second:\n",
    "                        is_second = True\n",
    "                        continue\n",
    "                    if l == '_':\n",
    "                        img_name = tile_file_name[0:i]\n",
    "                        tile_name = tile_file_name[i+1:]\n",
    "                        break\n",
    "\n",
    "                lbp_feature = lbp_df.loc[\n",
    "                    (lbp_df.iloc[:,0] == img_name) &\n",
    "                    (lbp_df.iloc[:,1] == tile_name)].iloc[0,:].to_numpy(copy=True)[2:]\n",
    "                hoc_feature = hoc_df.loc[\n",
    "                    hoc_df.iloc[:,0] == img_name].iloc[0,:].to_numpy(copy=True)[1:]\n",
    "                feature_vector = np.concatenate((lbp_feature, hoc_feature)).astype(float)\n",
    "\n",
    "                self.features.append(feature_vector)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_pths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_pths[idx]\n",
    "        image = Image.open(img_name)\n",
    "        catid = self.catids[idx]\n",
    "        if self.feature_transform:\n",
    "            # Currently not applying any transformations on features. \n",
    "            # They are already normalized.\n",
    "            # feature = self.feature_transform(self.features[idx])\n",
    "            feature = torch.from_numpy(self.features[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, feature, catid, img_name\n",
    "\n",
    "\n",
    "class STRIPKaggleDataLoader(object):\n",
    "    def __init__(self, train_dir, test_dir, batch_size, lbp_df, hoc_df, train_tsfm, \n",
    "                 feature_tsfm):\n",
    "        train_data = STRIPKaggleDataset(train_dir, lbp_df, hoc_df, \n",
    "                                        transform=train_tsfm,\n",
    "                                        feature_transform=feature_tsfm)\n",
    "        random_indices = np.random.permutation(len(train_data))\n",
    "        train_ratio, valid_ratio = 0.8, 0.2\n",
    "        n = int(train_ratio*len(train_data))\n",
    "        train_indices = random_indices[:n]\n",
    "        valid_indices = random_indices[n:]\n",
    "\n",
    "        # Split the dataset to training and validation datasets\n",
    "        train_dataset = torch.utils.data.Subset(train_data, train_indices)\n",
    "        valid_dataset = torch.utils.data.Subset(train_data, valid_indices)\n",
    "        test_dataset = STRIPKaggleDataset(test_dir, lbp_df, hoc_df,\n",
    "                                          transform=train_tsfm, # same transform as train\n",
    "                                          feature_transform=feature_tsfm)\n",
    "        self._dataloaders = load_train_valid_test_datasets(train_dataset,\n",
    "                                                           valid_dataset,\n",
    "                                                           test_dataset,\n",
    "                                                           batch_size,\n",
    "                                                           num_workers=0)\n",
    "        self._dataset_sizes = {'train': len(train_dataset),\n",
    "                               'valid': len(valid_dataset),\n",
    "                               'test': len(test_dataset)}\n",
    "\n",
    "    @property\n",
    "    def dataloaders(self):\n",
    "        return self._dataloaders\n",
    "\n",
    "    @property\n",
    "    def dataset_sizes(self):\n",
    "        return self._dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09bbf6af-f684-4865-8eb4-4a16a09d9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, device, \n",
    "                dataset_sizes, log_dir):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        logger.info('-' * 10)\n",
    "        \n",
    "        # Training and validation phase for each epoch\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for img, feature, labels, _ in dataloaders[phase]:\n",
    "                img = img.to(device, dtype=torch.float)\n",
    "                feature = feature.to(device, dtype=torch.float)\n",
    "                labels = labels.to(device, dtype=torch.int64)\n",
    "\n",
    "                # PyTorch accumulates the gradients on subsequent backward passes.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward. Track history if only in train.\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if phase == 'train':\n",
    "                        outputs, _ = model(img, feature) # model.forward() + additional hooks\n",
    "                    else:\n",
    "                        outputs = model(img, feature)\n",
    "                    _, preds = torch.max(outputs, dim=1, keepdim=False)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Stats\n",
    "                running_loss += loss.item() * img.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Training_loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Training_acc', epoch_acc, epoch)\n",
    "                \n",
    "            elif phase == 'valid':\n",
    "                writer.add_scalar('Valid_loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid_acc', epoch_acc, epoch)\n",
    "\n",
    "            logger.info('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            writer.close()\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    logger.info('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    logger.info('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def calculate_test_acc(model, testloader, device):\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    time_elapsed = 0\n",
    "    operation_times = []\n",
    "    id2cat = {0: 'CE', 1: 'LAA'}\n",
    "    all_img_names = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    logger.info('Calculating test accuracy')\n",
    "    for img, feature, labels, img_names in testloader:\n",
    "        img = img.to(device, dtype=torch.float)\n",
    "        feature = feature.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.int64)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            since = time.time()\n",
    "            outputs = model(img, feature)\n",
    "            _, preds = torch.max(outputs, dim=1, keepdim=False)\n",
    "            all_img_names += [x.split('/')[-1].removesuffix('.png') for x in list(img_names)]\n",
    "            all_labels += [id2cat[catid] for catid in labels.tolist()]\n",
    "            preds_list = [id2cat[catid] for catid in preds.tolist()]\n",
    "            all_preds += preds_list\n",
    "            operation_times.append(time.time() - since)\n",
    "        corrects += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "    detection_time = np.mean(operation_times)\n",
    "    detection_time_sd = np.std(operation_times)\n",
    "    msg = ('Detection time in millisecond for each image in test set '\n",
    "           'Mean: {:10.9f}\\t STD: {}')\n",
    "    logger.info(msg.format(detection_time * 1000, detection_time_sd * 1000))\n",
    "    test_acc = corrects.double() /total\n",
    "    return test_acc, all_img_names, all_labels, all_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9bc0a03-d9d2-4eb4-91bf-0fca0f96e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformations():\n",
    "    train_tsfm = tsfm.Compose([\n",
    "        tsfm.Resize((299,299), antialias=False), # required for inception-v3  \n",
    "        tsfm.PILToTensor(),\n",
    "    ])\n",
    "    feature_tsfm = tsfm.Compose([tsfm.ToTensor()])\n",
    "    return train_tsfm, feature_tsfm # left out test_tsfm for the purpose of the project\n",
    "\n",
    "def run(model, train_dir, test_dir, batch_size, num_epochs, lbp_df, hoc_df, device, \n",
    "        log_dir):\n",
    "    train_tsfm, feature_tsfm = get_transformations()\n",
    "    \n",
    "    loader = STRIPKaggleDataLoader(train_dir, test_dir, batch_size, lbp_df, hoc_df, \n",
    "                                   train_tsfm, feature_tsfm)\n",
    "    dataloaders = loader.dataloaders\n",
    "    dataset_sizes = loader.dataset_sizes\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model = train_model(model, dataloaders, criterion, optimizer, num_epochs, device,\n",
    "                        dataset_sizes, log_dir)\n",
    "    return model, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44710c47-6b3c-458f-b5d6-dc5ca3d31564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Number of parameters: 24615780\n",
      "INFO:__main__:Epoch 0/19\n",
      "INFO:__main__:----------\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:472: UserWarning: Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\n",
      "  warnings.warn(\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\")\n",
      "INFO:__main__:train Loss: 0.6289 Acc: 0.6830\n",
      "INFO:__main__:valid Loss: 0.5986 Acc: 0.6823\n",
      "INFO:__main__:Epoch 1/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.6116 Acc: 0.6951\n",
      "INFO:__main__:valid Loss: 0.6072 Acc: 0.7018\n",
      "INFO:__main__:Epoch 2/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.6012 Acc: 0.7043\n",
      "INFO:__main__:valid Loss: 0.9026 Acc: 0.7054\n",
      "INFO:__main__:Epoch 3/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.6031 Acc: 0.7038\n",
      "INFO:__main__:valid Loss: 0.9999 Acc: 0.6960\n",
      "INFO:__main__:Epoch 4/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5972 Acc: 0.7045\n",
      "INFO:__main__:valid Loss: 0.5845 Acc: 0.7191\n",
      "INFO:__main__:Epoch 5/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5951 Acc: 0.7072\n",
      "INFO:__main__:valid Loss: 0.6031 Acc: 0.7206\n",
      "INFO:__main__:Epoch 6/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5960 Acc: 0.7038\n",
      "INFO:__main__:valid Loss: 0.5910 Acc: 0.7206\n",
      "INFO:__main__:Epoch 7/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5917 Acc: 0.7090\n",
      "INFO:__main__:valid Loss: 0.5948 Acc: 0.7199\n",
      "INFO:__main__:Epoch 8/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5928 Acc: 0.7097\n",
      "INFO:__main__:valid Loss: 0.5815 Acc: 0.7119\n",
      "INFO:__main__:Epoch 9/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5981 Acc: 0.7047\n",
      "INFO:__main__:valid Loss: 0.5974 Acc: 0.6968\n",
      "INFO:__main__:Epoch 10/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5930 Acc: 0.7101\n",
      "INFO:__main__:valid Loss: 0.5727 Acc: 0.7112\n",
      "INFO:__main__:Epoch 11/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5844 Acc: 0.7122\n",
      "INFO:__main__:valid Loss: 0.5728 Acc: 0.7191\n",
      "INFO:__main__:Epoch 12/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5895 Acc: 0.7097\n",
      "INFO:__main__:valid Loss: 0.5649 Acc: 0.7256\n",
      "INFO:__main__:Epoch 13/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5829 Acc: 0.7132\n",
      "INFO:__main__:valid Loss: 0.8783 Acc: 0.7191\n",
      "INFO:__main__:Epoch 14/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5910 Acc: 0.7103\n",
      "INFO:__main__:valid Loss: 0.5657 Acc: 0.7249\n",
      "INFO:__main__:Epoch 15/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5852 Acc: 0.7110\n",
      "INFO:__main__:valid Loss: 0.5648 Acc: 0.7213\n",
      "INFO:__main__:Epoch 16/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5867 Acc: 0.7141\n",
      "INFO:__main__:valid Loss: 0.5640 Acc: 0.7256\n",
      "INFO:__main__:Epoch 17/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5863 Acc: 0.7132\n",
      "INFO:__main__:valid Loss: 0.5823 Acc: 0.7148\n",
      "INFO:__main__:Epoch 18/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5893 Acc: 0.7086\n",
      "INFO:__main__:valid Loss: 0.5809 Acc: 0.6996\n",
      "INFO:__main__:Epoch 19/19\n",
      "INFO:__main__:----------\n",
      "INFO:__main__:train Loss: 0.5925 Acc: 0.7068\n",
      "INFO:__main__:valid Loss: 0.5709 Acc: 0.7227\n",
      "INFO:__main__:Training complete in 25m 15s\n",
      "INFO:__main__:Best val Acc: 0.725632\n",
      "INFO:__main__:Calculating test accuracy\n",
      "INFO:__main__:Detection time in millisecond for each image in test set Mean: 31.137107075\t STD: 1.7980656145831426\n",
      "INFO:__main__:\n",
      "Test Acc: 0.644689\n"
     ]
    }
   ],
   "source": [
    "SEED = 61\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "proj_base = '/student/anv309/cmpt400'\n",
    "log_dir = proj_base + '/log/' + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_dir = proj_base + '/tiles_prod_train_cats'\n",
    "test_dir = proj_base + '/tiles_prod_test_cats'\n",
    "# TODO: Set abs path.\n",
    "# TODO: Need to combine the img and tile names from hoc-prod-tiles-sorted.csv\n",
    "#       with the normalized data from hoc-prod-data-norm.csv.\n",
    "csv_base = '/student/anv309/cmpt400/tiles_prod_csv'\n",
    "lbp_csv_path = csv_base + '/' + 'lbp-prod-tiles-sorted-norm-clean.csv'\n",
    "hoc_csv_path = csv_base + '/' + 'hoc-prod-sorted-norm.csv'\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "\n",
    "handcrafted_feature_size = 66 # hoc: 30, lbp: 36\n",
    "\n",
    "lbp_df = pd.read_csv(lbp_csv_path, header=None)\n",
    "hoc_df = pd.read_csv(hoc_csv_path, header=None)\n",
    "\n",
    "model = inception_v3STRIP(weights=Inception_V3_Weights.DEFAULT,\n",
    "                          handcrafted_feature_size=handcrafted_feature_size)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info('Number of parameters: {}'.format(num_params))\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, loader = run(model, train_dir, test_dir, batch_size, num_epochs, lbp_df, hoc_df, \n",
    "                    device, log_dir)\n",
    "\n",
    "testloader = loader.dataloaders['test']\n",
    "test_acc, tile_names, labels, preds = calculate_test_acc(model, testloader, device)\n",
    "img_names = [x[0:8] for x in tile_names]\n",
    "patient_ids = [x[0:6] for x in tile_names]\n",
    "logger.info('\\nTest Acc: {:4f}'.format(test_acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41618c5-097d-43e9-b6dc-9910030d0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_predictions_df\n",
      "  patient_id image_name                  tile_name label prediction\n",
      "0     026c97   026c97_0    026c97_0_tile_4096_4096    CE        LAA\n",
      "1     026c97   026c97_0    026c97_0_tile_4096_8192    CE         CE\n",
      "2     032f10   032f10_0       032f10_0_tile_0_4096    CE         CE\n",
      "3     032f10   032f10_0      032f10_0_tile_20480_0    CE         CE\n",
      "4     032f10   032f10_0   032f10_0_tile_20480_4096    CE         CE\n",
      "5     032f10   032f10_0    032f10_0_tile_4096_4096    CE         CE\n",
      "6     037300   037300_0  037300_0_tile_16384_12288    CE         CE\n",
      "7     037300   037300_0  037300_0_tile_20480_12288    CE         CE\n",
      "8     037300   037300_0  037300_0_tile_20480_16384    CE         CE\n",
      "9     037300   037300_0  037300_0_tile_24576_16384    CE         CE\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe containing tile classification results and labels\n",
    "test_predictions_df = pd.DataFrame({'patient_id': patient_ids,\n",
    "                                    'image_name': img_names,\n",
    "                                    'tile_name': tile_names,\n",
    "                                    'label': labels,\n",
    "                                    'prediction': preds})\n",
    "print('test_predictions_df')\n",
    "print(test_predictions_df.iloc[0:10, :])\n",
    "test_predictions_df.to_csv('/student/anv309/cmpt400/tiles_prod_csv/test_tile_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92d64b4-efb1-4c3b-8474-a2e120cad56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 704\n",
      "total: 1092\n",
      "accuracy: 0.6446886446886447\n",
      "   patient_id image_name                  tile_name label prediction\n",
      "1      026c97   026c97_0    026c97_0_tile_4096_8192    CE         CE\n",
      "2      032f10   032f10_0       032f10_0_tile_0_4096    CE         CE\n",
      "3      032f10   032f10_0      032f10_0_tile_20480_0    CE         CE\n",
      "4      032f10   032f10_0   032f10_0_tile_20480_4096    CE         CE\n",
      "5      032f10   032f10_0    032f10_0_tile_4096_4096    CE         CE\n",
      "6      037300   037300_0  037300_0_tile_16384_12288    CE         CE\n",
      "7      037300   037300_0  037300_0_tile_20480_12288    CE         CE\n",
      "8      037300   037300_0  037300_0_tile_20480_16384    CE         CE\n",
      "9      037300   037300_0  037300_0_tile_24576_16384    CE         CE\n",
      "10     037300   037300_0   037300_0_tile_4096_12288    CE         CE\n",
      "11     037300   037300_0   037300_0_tile_4096_16384    CE         CE\n",
      "12     037300   037300_0   037300_0_tile_4096_20480    CE         CE\n",
      "13     037300   037300_0  037300_0_tile_49152_12288    CE         CE\n",
      "14     037300   037300_0  037300_0_tile_49152_20480    CE         CE\n",
      "15     037300   037300_0  037300_0_tile_53248_20480    CE         CE\n",
      "16     037300   037300_0      037300_0_tile_57344_0    CE         CE\n",
      "17     037300   037300_0  037300_0_tile_57344_12288    CE         CE\n",
      "18     037300   037300_0  037300_0_tile_57344_16384    CE         CE\n",
      "19     037300   037300_0  037300_0_tile_57344_20480    CE         CE\n",
      "20     037300   037300_0      037300_0_tile_61440_0    CE         CE\n"
     ]
    }
   ],
   "source": [
    "correct_df = test_predictions_df[test_predictions_df['label'] == test_predictions_df['prediction']]\n",
    "print('corrects:', len(correct_df))\n",
    "print('total:', len(test_predictions_df))\n",
    "print('accuracy:', len(correct_df) / len(test_predictions_df))\n",
    "print(correct_df.iloc[0:20,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41178247-d80e-46de-87c2-6e131aa4c64b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of tiles classified as CE and LAA for each image.\n",
    "# Create a CE_count and LAA_count column to store these values.\n",
    "test_predictions_df['CE_count'] = 0\n",
    "test_predictions_df['LAA_count'] = 0\n",
    "for img_name in img_names:\n",
    "    img_df = test_predictions_df[test_predictions_df['image_name'] == img_name]\n",
    "    test_predictions_df.loc[test_predictions_df['image_name'] == img_name, 'CE_count'] = len(img_df[img_df['prediction'] == 'CE'])\n",
    "    test_predictions_df.loc[test_predictions_df['image_name'] == img_name, 'LAA_count'] = len(img_df[img_df['prediction'] == 'LAA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af0b734b-3972-420c-8302-c8a37fd7a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'final_pred' column that represents the \"voted\" final classification result\n",
    "# for an image (using classification results of its constituent tiles).\n",
    "# Add columns to store the probability that an image is of type CE / LAA based on\n",
    "# tile classification results.\n",
    "image_counts_df = test_predictions_df.iloc[:, [1,3,5,6]]\n",
    "image_counts_df = image_counts_df.drop_duplicates(subset=['image_name'])\n",
    "image_counts_df['final_pred'] = ''\n",
    "image_counts_df['CE_p'] = 0\n",
    "image_counts_df['LAA_p'] = 0\n",
    "for image_name in img_names:\n",
    "    img_row = image_counts_df.loc[(image_counts_df['image_name'] == image_name)]\n",
    "    ce_count = int(img_row['CE_count'])\n",
    "    laa_count = int(img_row['LAA_count'])\n",
    "    if ce_count > laa_count:\n",
    "        image_counts_df.loc[image_counts_df['image_name'] == image_name, 'final_pred'] = 'CE'\n",
    "    else:\n",
    "        image_counts_df.loc[image_counts_df['image_name'] == image_name, 'final_pred'] = 'LAA'\n",
    "    image_counts_df.loc[image_counts_df['image_name'] == image_name, 'CE_p'] = ce_count / (ce_count + laa_count)\n",
    "    image_counts_df.loc[image_counts_df['image_name'] == image_name, 'LAA_p'] = laa_count / (ce_count + laa_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03cd60e0-df9b-48c1-9631-ee01b30abd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     image_name label  CE_count  LAA_count final_pred      CE_p     LAA_p\n",
      "847    497c27_0   LAA         4          0         CE  1.000000  0.000000\n",
      "851    54838a_0   LAA         7          0         CE  1.000000  0.000000\n",
      "858    69d655_0   LAA        10          1         CE  0.909091  0.090909\n",
      "869    862501_0   LAA        11          0         CE  1.000000  0.000000\n",
      "880    862501_1   LAA         9          0         CE  1.000000  0.000000\n",
      "889    9f7649_0   LAA         6          0         CE  1.000000  0.000000\n",
      "895    9f7649_1   LAA         9          0         CE  1.000000  0.000000\n",
      "904    a26055_0   LAA         8          0         CE  1.000000  0.000000\n",
      "912    a26055_1   LAA        15          0         CE  1.000000  0.000000\n",
      "927    a26055_2   LAA        22          0         CE  1.000000  0.000000\n",
      "949    a6f5ae_0   LAA         1          0         CE  1.000000  0.000000\n",
      "950    b6f1e9_0   LAA        11          0         CE  1.000000  0.000000\n",
      "961    bc9536_0   LAA         7          0         CE  1.000000  0.000000\n",
      "968    c5d171_0   LAA        13         10         CE  0.565217  0.434783\n",
      "991    c9ab6c_0   LAA        54          0         CE  1.000000  0.000000\n",
      "1045   d8db68_0   LAA         7          0         CE  1.000000  0.000000\n",
      "1052   defd00_0   LAA        13          1         CE  0.928571  0.071429\n",
      "1066   e3e850_0   LAA        15          1         CE  0.937500  0.062500\n",
      "1082   eb21f0_0   LAA         2          4        LAA  0.333333  0.666667\n",
      "1088   ed5006_0   LAA         4          0         CE  1.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# print(image_counts_df.iloc[-30:,:])\n",
    "print(image_counts_df[image_counts_df['label'] == 'LAA'].iloc[-20:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5250abe8-a7bf-4d45-af8e-ced5470572d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 74\n",
      "total: 105\n",
      "accuracy: 0.7047619047619048\n",
      "     image_name label  CE_count  LAA_count final_pred      CE_p     LAA_p\n",
      "889    9f7649_0   LAA         6          0         CE  1.000000  0.000000\n",
      "895    9f7649_1   LAA         9          0         CE  1.000000  0.000000\n",
      "904    a26055_0   LAA         8          0         CE  1.000000  0.000000\n",
      "912    a26055_1   LAA        15          0         CE  1.000000  0.000000\n",
      "927    a26055_2   LAA        22          0         CE  1.000000  0.000000\n",
      "949    a6f5ae_0   LAA         1          0         CE  1.000000  0.000000\n",
      "950    b6f1e9_0   LAA        11          0         CE  1.000000  0.000000\n",
      "961    bc9536_0   LAA         7          0         CE  1.000000  0.000000\n",
      "968    c5d171_0   LAA        13         10         CE  0.565217  0.434783\n",
      "991    c9ab6c_0   LAA        54          0         CE  1.000000  0.000000\n",
      "1045   d8db68_0   LAA         7          0         CE  1.000000  0.000000\n",
      "1052   defd00_0   LAA        13          1         CE  0.928571  0.071429\n",
      "1066   e3e850_0   LAA        15          1         CE  0.937500  0.062500\n",
      "1082   eb21f0_0   LAA         2          4        LAA  0.333333  0.666667\n",
      "1088   ed5006_0   LAA         4          0         CE  1.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Get classification accuracy for images (not tiles)\n",
    "img_correct_df = image_counts_df[image_counts_df['label'] == image_counts_df['final_pred']]\n",
    "print('corrects:', len(img_correct_df))\n",
    "print('total:', len(image_counts_df))\n",
    "print('accuracy:', len(img_correct_df) / len(image_counts_df))\n",
    "print(image_counts_df.iloc[-15:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ed70484-88f8-4df7-a5e0-a1f64a91bb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143 0.2857142857142857 105 75 30\n"
     ]
    }
   ],
   "source": [
    "# calculate weights\n",
    "ce_count = len(image_counts_df[image_counts_df['label'] == 'CE'])\n",
    "laa_count = len(image_counts_df[image_counts_df['label'] == 'LAA'])\n",
    "weights = {'CE': ce_count / (ce_count + laa_count),\n",
    "           'LAA': laa_count / (ce_count + laa_count)}\n",
    "\n",
    "print(weights['CE'], weights['LAA'], ce_count + laa_count, ce_count, laa_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59171166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle competition loss. See https://www.kaggle.com/competitions/mayo-clinic-strip-ai/overview/evaluation.\n",
    "loss = 0\n",
    "\n",
    "for _, row in image_counts_df.iterrows():\n",
    "    p = 0\n",
    "    label = row['label']\n",
    "    if label == 'CE':\n",
    "        p = row['CE_p']\n",
    "    else:\n",
    "        p = row['LAA_p']\n",
    "    p = max(min(p, 1 - 10**(-15)), 10**(-15))\n",
    "    loss -= weights[label] * math.log(p)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee46cae2-8964-4e74-aef7-9800cf3f3dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5faf0fb-4c01-4108-8013-f1240df27389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-37a9d69b7df06e5c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-37a9d69b7df06e5c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 40404;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --port=40404 --logdir /student/anv309/cmpt400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6f49c-0860-4968-aa58-99a936c51f31",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer.add_scalar(training_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
